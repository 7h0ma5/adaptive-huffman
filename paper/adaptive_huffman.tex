\documentclass[twoside,11pt,a4paper]{article}

\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{latexsym,amsmath,amssymb}
\usepackage[ngerman]{babel}
\usepackage{theorem}
\usepackage{dcolumn}
\usepackage{url}
\usepackage{tikz}
\usepackage{caption}
\usepackage{subcaption}
\usepackage[hidelinks]{hyperref}
\usetikzlibrary{trees,calc}

\newcommand{\Frac}[2]{\frac{\displaystyle #1}{\displaystyle #2}}
\newlength{\textwd}
\newlength{\oddsidemargintmp}
\newlength{\evensidemargintmp}
\newcommand{\hspaceof}[2]{\settowidth{\textwd}{#1}\mbox{\hspace{#2\textwd}}}
\newlength{\textht}
\newcommand{\vspaceof}[3]{\settoheight{\textht}{#1}\mbox{\raisebox{#2\textht}{#3}}}
\newcommand{\PreserveBackslash}[1]{\let\temp=\\#1\let\\=\temp}

\newenvironment{deflist}[1][\quad]%
{  \begin{list}{}{%
      \renewcommand{\makelabel}[1]{\textbf{##1}\hfil}%
      \settowidth{\labelwidth}{\textbf{#1}}%
      \setlength{\leftmargin}{\labelwidth}
      \addtolength{\leftmargin}{\labelsep}}}
{  \end{list}}

\newenvironment{Quote}% Definition of Quote
{  \begin{list}{}{%
      \setlength{\rightmargin}{0pt}}
      \item[]\ignorespaces}
{\unskip\end{list}}

\newtheorem{Cor}{Corollary}
\theoremstyle{break}
\theorembodyfont{\itshape}
\newtheorem{Def}[Cor]{Definition}
\theoremheaderfont{\scshape}

\pagestyle{headings}

\textwidth 15cm
\textheight 22cm
\oddsidemargin 1cm
\evensidemargin 0cm

\tikzset{
grow=down,
level 1/.style={sibling distance=2.5cm, level distance=1.3cm},
level 2/.style={sibling distance=2.0cm, level distance=1.3cm},
level 3/.style={sibling distance=1.5cm, level distance=1.3cm},
level 4/.style={sibling distance=1.0cm, level distance=1.3cm},
edge from parent path={
(\tikzparentnode) |-
($(\tikzparentnode)!0.5!(\tikzchildnode)$) -|
(\tikzchildnode)}
}

\tikzstyle{end} = [circle, minimum width=5pt, fill, inner sep=0pt]

\renewcommand*\ttdefault{txtt}

\usepackage{parskip}
%\setlength{\parindent}{0pt}
%\setlength{\parskip}{2ex plus 0.5ex minus 0.2ex}

%\setlength{\parindent}{0pt}
%\setlength{\parskip}{1ex plus 0.5ex minus 0.2ex}

\usepackage{titlesec}
\titlespacing\section{0pt}{12pt plus 4pt minus 2pt}{0pt plus 2pt minus 2pt}
\titlespacing\subsection{0pt}{12pt plus 4pt minus 2pt}{0pt plus 2pt minus 2pt}
\titlespacing\subsubsection{0pt}{12pt plus 4pt minus 2pt}{0pt plus 2pt minus 2pt}

\begin{document}
%\raggedbottom

\pagestyle{empty}
\begin{center}
    Rheinisch-Westfälische Technische Hochschule Aachen \\
    Lehrstuhl für Informatik VI \\
    Prof. Dr.-Ing. Hermann Ney\\[6ex]
    Proseminar Datenkompression im WS 2014/2015\\[12ex]

    \LARGE
    \textbf{Adaptive Huffman-Kodierung und Anwendungen} \\[6ex]
    \textit{Thomas Gatzweiler} \\[6ex]
    \Large
    Matrikelnummer 318947 \\[6ex]
    12. November 2014

    \vfill
    \Large Betreuer: Patrick Dötsch
\end{center}

\newpage
\
\newpage

\pagestyle{headings}
\tableofcontents
\listoftables
\listoffigures
\newpage
\pagestyle{empty}
\newpage
\pagestyle{headings}


\newcommand{\sectionbreak}{\clearpage}

% Literaturverzeichnis wie in der .bib Datei ordnen
\nocite{*}

\section{Einleitung}
Bei einer Datenübertragung mit eingeschränkter Bandbreite ist eine
Komprimierung der Daten von großem Vorteil.

Ein Symbol kann ein eine Bitfolge, ein Buchstabe, ein Wort oder auch
eine Kombination von Wörtern sein. Wie ein Symbol definiert ist, ist
vom Anwendungsfall abhängig. Es kann beispielsweise mehr Sinn machen,
bei einem Text die Wörter als Symbole zu definieren, als die
Buchstaben. Die Menge an Symbolen, die für eine Datenmenge
exisitiert, wird als Alphabet bezeichnet.

Ein Problem der Huffman-Kodierung ist, dass die Wahrscheinlichkeiten
der zu kodierenden Symbole vor der Kodierung bekannt sein müssen. Wenn
die zu komprimierenden Daten lokal verfügbar sind, ist es möglich,
zuerst die Häufigkeiten der vorkommenden Symbole zu analysieren und
dann die Daten zu kodieren. Handelt es sich aber um einen laufenden
Datenstrom, ist es nicht möglich die Häufigkeiten im vorraus zu
ermitteln. Eine Lösung für dieses Problem bietet die adaptive
Huffman-Kodierung, die sich laufend an die Häufigkeiten der Symbole
anpasst.

%Bei der Konstruktion von Codes mit variabler Länge sind zwei Punkte
%wichtig, die Codewörter sollten eindeutig sein und sie sollten den
%Symbolen so zugewiesen werden, dass die Codewörter in der
%resultierende Kodierung die minimale mittlere Länge haben.

\section{Huffman-Kodierung} \label{sec:huffman}
Dieser Abschnitt gibt eine Einführung in die Hufman-Kodierung, welche
die Grundlage für die adaptive Huffman-Kodierung ist. Der Abschnitt
basiert auf \cite{Salomon:2010} und \cite{Sayood:2006}.

Die Huffman-Kodierung ist eine stochastische Kodierung, das heißt, sie
kodiert Daten basierend auf der Häufigkeit der vorkommenden Symbole
des Eingabealphabets. Sie wurde 1952 von David Huffman entwickelt und
ist seitdem Gegenstand vieler Forschungsarbeiten.

Es gibt Fälle, in denen durch die Huffman-Kodierung keine
Komprimierung erziehlt werden kann, zum Beispiel bei Symbolen mit
gleicher Häufigkeit, da es sich dabei stochastisch gesehen um
Zufallsdaten handelt. Beispielsweise ist eine Zeichenkette
$a_1a_1{\dots}a_1a_2a_2{\dots}a_2a_3a_3{\dots}a_3$ mit der
Huffman-Kodierung nicht komprimierbar. Bei einem Alphabet, welches nur
aus zwei Symbolen besteht, kann die Huffman-Kodierung auch nicht zur
Komprimierung verwendet werden. Einem Symbol würde in diesem Fall
entweder die {\tt1} oder die {\tt0} als Codewort zugewiesen werden.
Da die Huffman-Kodierung keine Codewörter vergeben kann, die kürzer
als ein Bit sind, ist es nicht möglich, dieses Alphabet zu verbessern.

Der Codewörter, die bei der Huffman-Kodierung erzeugt werden, sind
immer präfixfrei, das heißt kein Codewort ist Präfix eines anderen
Codewortes. Das macht es möglich, die erzeugten Codewörter ohne
Trennzeichen direkt hintereinander zu schreiben und beim Dekodieren
eindeutig zu erkennen.

Die Huffman-Kodierung basiert auf zwei Beobachtungen über optimale
präfixfreie Codes:

\begin{enumerate}
\item In einem optimalen Code haben die Symbole, die häufiger
  auftreten, also eine höhere Auftrittswahrscheinlichkeit haben,
  kürzere Codewörter als Symbole, die seltener auftreten.
\item In einem optimalen Code haben die Codewörter für die beiden
  Symbole, die am seltensten auftreten, die selbe Länge.
\end{enumerate}

Die erste Beobachtung ist offensichtlich korrekt. Wäre die Länge des
Codeworts eines seltener auftretenden Symbols kürzer als die eines
häufiger auftretenden Symbols, dann wäre die mittlere Länge eines
Codeworts länger, weshalb ein solcher Code nicht optimal sein kann.

Die zweite Beobachtung lässt sich einfach zeigen. Angenommen es gäbe
einen optimalen Code, bei dem die zwei am seltensten auftretenden
Symbole Codewörter mit unterschiedlicher Länge mit dem
Längenunterschied $k$ zugeweisen bekommen. Weil es sich um einen
präfixfreien Code handelt, kann das kürzere Codewort kein Präfix des
längeren Codeworts sein. Das bedeutet man kann die letzten $k$ Bits
des längeren Codeworts wegfallen lassen und die beiden Codewörter
wären immer noch eindeutig. Da es sich bei den Symbolen der zwei
Codewörter um die am seltensten Symbole eines Alphabets handelt, gibt
es keine längeren Codewörter. Aus diesem Grund ist es nicht möglich,
dass das gekürzte Codewort Präfix eines längeren Codeworts
wird. Außerdem ensteht durch das Entfernen dieser $k$ Bits ein neuer
Code, der eine kürzere mittlere Codewortlänge hat und somit besser ist
als der Ausgangscode. Das ist ein Widerspruch zu der Annahme, dass es
sich bei dem Ausgangscode um einen optimalen Code handelt. Die zweite
Feststellung trifft somit auch zu.

Die Huffman-Kodierung erweitert diese Beobachtungen um die Bedingung,
dass sich die Codewörter der zwei seltensten Symbole nur beim letzten
Bit unterscheiden dürfen.

\subsection{Huffman-Baum}
Um den Symbolen des Eingangsalphabets Codewörter zuzuweisen, wird der
sogenannte \emph{Huffman-Baum} aufgebaut. Es handelt sich dabei um
einen Binärbaum, damit die entstehenden Codewörter binär sind.

Um den Huffman-Baum aufzubauen muss die Wahrscheinlichkeitsverteilung
für die Symbole des Eingabealphabets bekannt sein. Die
Wahrscheinlichkeiten können entweder geschätzt werden, wie zum
Beispiel mit den Buchstabenhäufigkeiten einer Sprache, oder sie müssen
in einem zusätzlichen Durchgang bestimmt werden.

Als nächstes wird die Liste der Symbole nach Häufigkeiten geordnet.
Dann Huffman-Baum wird von unten nach oben aufgebaut, also von den
Blättern zur Wurzel. Es werden immer die beiden Symbole mit der
niedrigsten Auftrittswahrscheinlichkeit aus der Liste entfernt und ein
Knoten erstellt, der diese beiden Symbole als Nachfolger hat. Dieser
Knoten wird dann als Hilfssymbol mit der summierten Wahrscheinlichkeit
wieder in der Liste eingeordnet. Der Huffman-Baum ist fertig, wenn die
Liste auf ein Hilfssymbol reduziert ist.

Beim Erstellen des Huffman-Baums gibt verschiedene Möglichkeiten, wenn
es Symbole mit den gleichen Wahrscheinlichkeiten gibt. Die Wahl einer
dieser Möglichkeiten hat einen Einfluss auf die erzeugten Codewörter,
aber nicht auf die mittlere Länge des Codes.

\begin{table}[h]
\centering
\caption{Beispiel für eine Häufigkeitsverteilung}

\begin{subtable}[t]{0.3\textwidth}
  \centering
  \caption{Ausgangstabelle}
  \begin{tabular}{c|r|r}
    $a_i$ & $H(a_i)$ & $P(a_i)$ \\ \hline
    A & 40   & 0.20 \\
    B & 4    & 0.02 \\
    C & 16   & 0.08 \\
    D & 20   & 0.10 \\
    E & 120  & 0.60 \\
  \end{tabular}
  \vspace{3ex}
  \label{tab:HBEX0}
\end{subtable}
\begin{subtable}[t]{0.3\textwidth}
  \centering
  \caption{Schritt 1}
  \begin{tabular}{c|c}
    $a_i$ & $P(a_i)$ \\ \hline
    E & 0.60 \\
    A & 0.20 \\
    D & 0.10 \\
    C & 0.08 \\
    B & 0.02 \\
  \end{tabular}
  \vspace{3ex}
  \label{tab:HBEX1}
\end{subtable}
\begin{subtable}[t]{0.3\textwidth}
  \centering
  \caption{Schritt 2}
  \begin{tabular}{c|c}
    $a_i$ & $P(a_i)$ \\ \hline
    E & 0.60 \\
    A & 0.20 \\
    D & 0.10 \\
    \{C, B\} & 0.10 \\ \\
  \end{tabular}
  \vspace{3ex}
  \label{tab:HBEX2}
\end{subtable}
\begin{subtable}[t]{0.3\textwidth}
  \centering
  \caption{Schritt 3}
   \begin{tabular}{c|c}
    $a_i$ & $P(a_i)$ \\ \hline
    E & 0.60 \\
    A & 0.20 \\
    \{B, C, D\} & 0.20 \\
  \end{tabular}
  \label{tab:HBEX3}
\end{subtable}
\begin{subtable}[t]{0.3\textwidth}
  \centering
  \caption{Schritt 4}
   \begin{tabular}{c|c}
    $a_i$ & $P(a_i)$ \\ \hline
    E & 0.60 \\
    \{A, B, C, D\} & 0.40 \\ \\
  \end{tabular}
  \label{tab:HBEX4}
\end{subtable}
\begin{subtable}[t]{0.3\textwidth}
  \centering
  \caption{Schritt 5}
  \begin{tabular}{c|c}
    $a_i$ & $P(a_i)$ \\ \hline
    \{A, B, C, D, E\} & 1.00 \\ \\ \\
  \end{tabular}
  \label{tab:HBEX5}
\end{subtable}
\label{tab:HBEX}
\end{table}

Am besten lässt sich die Erzeugung eines Huffman-Baums mit Hilfe eines
Beispiels verstehen. In Tabelle~\ref{tab:HBEX0} ist ein Beispiel einer
Häufigkeitsverteilung dargestellt, mit den Symbolen $\{a_i \mid i \in
[1, 5]\}$, den absoluten Häufigkeiten der Symbole, $H(a_i)$, und die
Wahrscheinlichkeiten der Symbole, $P(a_i)$.

Nachdem alle Symbole nach den Wahrscheinlichkeiten geordnet sind,
ergibt sich Tabelle~\ref{tab:HBEX1}. Die am seltensten Vorkommenden
Symbole sind $C$ und $B$, also wird für $C$ und $B$ jeweils ein
Blattknoten erzeugt. Dann wird ein Elternknoten $\{C, B\}$ erzeugt,
der die summierte Wahrscheinlichkeit beider Symbole von $0.10$
besitzt. Für diesen Knoten wird das Hilfssymbol $\{C, B\}$ zurück in
die Liste geschrieben und es ergibt sich Tabelle~\ref{tab:HBEX2}.

Nun sind die seltensten Symbole in der Liste das Hilfssymbol $\{C,
B\}$ und das Symbol $D$. Für $D$ wird wieder ein Blattknoten erzeugt,
für $\{C, B\}$ existiert bereits ein Knoten. Dann wird ein
Elternknoten $\{B, C, D\}$ erzeugt, der als Nachfolger den Blattknoten
für $D$ und den Knoten $\{C, B\}$ hat. Dieser wird als Hilfssymbol
$\{B, C, D\}$ mit der summierten Wahrscheinlichkeit von $0.20$ in die
Liste geschrieben und es ergibt sich Tabelle~\ref{tab:HBEX3}.

Der gleiche Prozess wird noch zwei mal wiederholt (siehe
Tabelle~\ref{tab:HBEX4} und \ref{tab:HBEX5}), danach existiert nur
noch ein Hilfssymbol in der Liste. Der Huffman-Baum ist nun fertig
und in Abbildung~\ref{fig:HBEX} dargestellt.

\begin{figure}[h]
\centering
\begin{tikzpicture}
\node {\{A, B, C, D, E\}}
child {
  node {E}
  edge from parent
  node[left] {\scriptsize $0.60$}
}
child {
  node {\{A, B, C, D\}}
  child {
    node {A}
    edge from parent
    node[left] {\scriptsize $0.20$}
  }
  child {
    node {\{B, C, D\}}
    child {
      node {D}
      edge from parent
      node[left] {\scriptsize $0.10$}
    }
    child {
      node {\{C, B\}}
      child {
        node {C}
        edge from parent
        node[left] {\scriptsize $0.08$}
      }
      child {
        node {B}
        edge from parent
        node[right] {\scriptsize $0.02$}
      }
      edge from parent
      node[right] {\scriptsize $0.10$}
    }
    edge from parent
    node[right] {\scriptsize $0.20$}
  }
  edge from parent
  node[right] {\scriptsize $0.40$}
};
\end{tikzpicture}
\caption{Huffman-Baum für die Verteilung aus Tabelle~\ref{tab:HBEX}} \label{fig:HBEX}
\end{figure}

% sibling-property

\subsection{Kodierung}
Nachdem der Huffman-Baum erzeugt wurde, kann er verwendet werden, um
die Codewörter für die Symbole zu bestimmen. Aus dem Pfad zu einem
Blatt lässt sich das Codewort für dieses Blatt ableiten.

In Abbildung~\ref{fig:HKOD} ist ein Huffman-Baum mit den Symbolen
\emph{H}, \emph{R}, \emph{T} und \emph{W} abgebildet. Um das Wort
\emph{RWTH} zu kodieren, wird nun für jedes Symbol der Pfad von der
Wurzel bis zu dem Blatt für dieses Symbol durchschritten. Dabei wird das
Codewort gebildet, indem bei jedem Knoten, der auf dem Weg erreicht
wird, eine {\tt1} an das Codewort gehängt, wenn der Knoten der linke
Nachfolger des vorherigen ist, und eine {\tt0} angehängt, wenn der
Knoten der rechte Nachfolger ist.

Für \emph{R} erhält man so das Codewort {\tt000}, für \emph{W} das
Codewort {\tt001}, für \emph{T} das Codewort {\tt1} und für \emph{H}
das Codewort \emph{\tt01}. Insgesamt wird \emph{RWTH} also zu
{\tt000001101} kodiert.

% Der beste Code ist der mit der kleinsten Varianz, da die Buffer nicht groß sein müssen

\begin{figure}[h]
\centering
\begin{tikzpicture}
\node {}
    child {
        node {\tt1}
        child {
            node {T}
            edge from parent
        }
        edge from parent
        node[left]  {$\frac{11}{20}$}
    }
    child {
        node {\tt0}
        child {
                node {\tt1}
                child {
                    node {H}
                    edge from parent
                }
                edge from parent
                node[left] {$\frac{5}{20}$}
            }
            child {
                node {\tt0}
                child {
                    node {\tt1}
                    child {
                        node {W}
                        edge from parent
                    }
                    edge from parent
                    node[left] {$\frac{3}{20}$}
                }
                child {
                    node {\tt0}
                    child {
                        node {R}
                        edge from parent
                    }
                    edge from parent
                    node[right] {$\frac{1}{20}$}
                }
                edge from parent
                node[right] {$\frac{4}{20}$}
            }
        edge from parent
        node[right] {$\frac{9}{20}$}
    };
\end{tikzpicture}
\caption{Kodierung mit Hilfe eines Huffman-Baums} \label{fig:HKOD}
\end{figure}

\subsection{Dekodierung} \label{sec:huffman-dec}
Die Dekodierung kann mit Hilfe des Huffman-Baums durchgeführt
werden. Damit der Dekodierer einen Huffman-Baum erzeugen kann, muss
entweder die Häufigkeitsverteilung oder der Huffman-Baum zusammen mit
den kodierten Daten gespeichert bzw. übertragen werden. Wenn dem
Dekoder nur die Häufigkeitsverteilung bekannt ist, muss er den
Huffman-Baum nach exakt dem gleichen Prinzip aufbauen, wie der
Kodierer, damit der selbe Huffman-Baum ensteht und die Daten korrekt
dekodiert werden können.

Die kodierten Daten werden ähnlich wie beim Kodieren durch das
traviersieren des Huffman-Baums dekodiert. Der Dekodierer startet bei
der Wurzel und geht für jedes gelesene Bit entweder zum linken oder
rechten Nachfolger des aktuellen Knotens. Sobald ein Blatt des Baums
erreicht wird, ist ein Symbol fertig dekodiert, der Dekodierer gibt
das im Blatt gespeicherte Symbol aus und kehrt zurück zur Wurzel des
Baums und verarbeitet das nächste Bit.

%\subsection{Mittlere Codewort-Länge}

%\subsection{Höhe des Huffman-Baums}
%Da die Höhe des Huffman-Baumes der maximalen Codewort länge
%entspricht, ist die Berechnung von Interesse.

\section{Adaptive Huffman-Kodierung}
Der folgende Abschnitt basiert auf \cite{Salomon:2010}.

Für die Huffman-Kodierung ist das Wissen über die
Wahrscheinlichkeitsverteilung notwendig der zu kodierenden Daten
notwendig, um die Codewörter für die Symbole des Eingabealphabets zu
bestimmen. Es gibt jedoch Fälle, in denen diese Verteilung im voraus
nicht bekannt ist, zum Beispiel bei der Übertragung von Echtzeitdaten
über ein Netzwerk. Auch ist Verfahren zur Kodierung einer Datei in nur
einem Durchgang wünschenswert. Normalerweise muss eine Datei zunächst
komplett gelesen werden, um die Wahrscheinlichkeiten der vorkommenden
Symbole zu bestimmen, aber besonders bei großen Datenmengen und
begrenztem Arbeitsspeicher ist dies ein erheblicher Rechenaufwand.

Die Methode zur Lösung dieser Problemstellung ist die \emph{adaptive
  Huffman-Kodierung}. Sie wurde ursprünglich 1973 von Newton Faller
und 1978 Robert Gallager unabhängig entwickelt und 1985 von Donald
Knuth (1985) wesentlich verbessert.

Auch die adaptive Huffman-Kodierung verwendet einen Huffman-Baum zur
Erzeugung der Codeworte. Da anfangs jedoch noch keine Häufigkeiten der
auftretenden Symbole und auch die Symbole selbst nicht bekannt sind,
beginnen Kodierer und Dekodierer mit einem leeren Baum. Von diesem
Punkt an aktualisieren Kodierer und Dekodierer den Baum mit jeden
neuen Wort entsprechend der neuen Häufigkeitsverteilung. Es ist
wichtig, dass Kodierer und Dekodierer die auf den Huffman-Baum
angewandeten Operationen exakt spiegeln und mit Mehrdeutigkeiten auf
die selbe Weise umgehen.

% Robustheit

\subsection{Kodierung}

Der Kodierer beginnt das Lesen der Eingabedaten mit einem leeren
Baum. Das erste gelesene Symbol gibt der Kodierer unkomprimiert aus
und fügt das Symbol dann in den Baum ein, wodurch ab diesem Punkt ein
Codewort für dieses Symbol exisitert. Wenn der Kodierer das nächste
mal auf dieses Symbol trifft, wird dieses Codewort ausgegeben und der
Häufigkeitszähler für dieses Symbol im Huffman-Baum inkrementiert.
Dabei kann es passieren, dass der Huffman-Baum nicht mehr das
Kriterium eines optimalen Codes erfüllt (Abschnitt~\ref{sec:huffman}).
Deshalb muss nach jedem Symbol überprüft werden, ob dies der Fall ist,
wenn ja muss der Huffman-Baum neu angeordnet werden, wodurch sich die
Codewörter ändern.

\subsection{Dekodierung}

Der Dekodierer vollzieht exakt die selben Schritte. Wenn er ein
unkomprimiertes Symbol liest, fügt er dies zu seinem Huffman-Baum
hinzu und hat so für das nächste Auftreten dieses Symbols das selbe
Codewort wie der Kodierer. Wenn er ein kodiertes Symbol -- also ein
Codewort -- liest, wird es, wie in Abschnitt~\ref{sec:huffman-dec}
beschrieben, dekodiert und ausgegeben. Zusätzlich wird der
Häufigkeitszähler für dieses Symbol inkrementiert und der Huffman-Baum
gegebenenfalls wie beim Kodierer neu angeordnet.

Nun stellt sich das Problem, dass der Dekodierer nicht zwischen
unkomprimierten Codewörtern unterscheiden kann, da es sich im beiden
Fällen nur um eine Bitfolge im Datenstrom handelt. Um diese Unklarheit
zu beseitigen wird jedes unkomprimierte Symbol mit einem speziell
reserviertem Codewort begonnen, dem sogenannten \emph{Escape-Code}.
Nach dem Escape-Code kann dann zum Beispiel ein einzelnes
ASCII-Zeichen folgen oder auch längere Bitfolgen, die entweder mit der
zu erwartenden Länge beginnen oder selbst mit einem eigenen
eindeutigen Escape-Code enden.

Weiterhin besteht das Problem, dass der Escape-Code kein Codewort
eines Symbols sein darf und auch kein Präfix davon. Erschwerend kommt
hinzu, dass sich die Codewörter durch die Neuanordnung des
Huffman-Baums laufend verändern können. Der natürlichste Weg ist also,
ein eigenes, leeres Blatt für den Escape-Code in den Huffman-Baum
einzufügen. Das bringt den Vorteil mit sich, dass sich die Länge des
Codeworts für den Escape-Code automatisch an die Häufigkeit des
auftretens anpasst. Je seltener ein zuvor unbekanntes Symbol auftritt,
welches unkomprimiert übertragen werden muss, desto länger ist das
Codewort für den Escape-Code im Vergleich zu den Codewörtern der
anderen Symbole.


\begin{figure}[h]
\centering
\begin{tikzpicture}
\node {}
  child {
        node {\tt1}
        child {
            node {A}
            edge from parent
        }
        edge from parent
        node[left]  {$\frac{11}{20}$}
    }
    child {
        node {\tt0}
        child {
                node {\tt1}
                child {
                    node {B}
                    edge from parent
                }
                edge from parent
                node[left] {$\frac{5}{20}$}
            }
            child {
                node {\tt0}
                child {
                    node {\tt1}
                    child {
                        node[end] {}
                        edge from parent
                    }
                    edge from parent
                    node[left] {$\frac{3}{20}$}
                }
                child {
                    node {\tt0}
                    child {
                        node {R}
                        edge from parent
                    }
                    edge from parent
                    node[right] {$\frac{1}{20}$}
                }
                edge from parent
                node[right] {$\frac{4}{20}$}
            }
        edge from parent
        node[right] {$\frac{9}{20}$}
    };
\end{tikzpicture}
\caption{Ein Huffman-Baum mit Platzhalter für den Escape-Code} \label{fig:HESC}
\end{figure}


\subsection{Aktualisierung des Huffman-Baums}
Der Huffman-Baum muss sowohl im Kodierer als auch im Dekodierer nach
jeder Aktualisierung eines Häufigkeitszählers überprüft werden und
dann, falls es sich nicht länger um einen Huffman-Baum handelt,
aktualisiert werden.

\subsection{Überlauf der Häufigkeitszähler}
Die Häufigkeitszähler für das Auftreten von Symbolen werden bei der
adaptiven Huffman-Kodierung als Integer gespeichert. Abhängig von der
Datenmenge kann es passieren, dass die auftretende Häufigkeit eines
Symbols die Grenze des darstellbaren Zahlenbereiches überschreitet --
es kommt zu einem Integerüberlauf.

Der darstellbare Zahlenbereich für Integer ist abhängig von der
verwendeten Hardware. Mit modernen 64-Bit Rechnerarchitekturen ist die
Wahrscheinlichkeit für einen Überlauf heute deutlich geringer, die
Möglichkeit sollte dennoch nicht vernachlässigt werden. Viele
Microcontroller rechnen standardmäßig mit 8- oder 16-Bit-Integern, die
nur 256 bzw. 65536 Werte darstellen können.

Eine Möglichkeit zu Verhinderung der Überläufe wäre die Benutzung
einer Biblitothek zur Speicherung von Integern mit dynamischer Größe,
wie zum Beispiel die \emph{GNU Multiple Precision Arithmetic
  Library}. Ein Nachteil ist der entstehende Overhead, außerdem ist
die Verwendung einer solchen Bibliothek bei vielen Microcontrollern
nicht möglich. \cite{GMP}

Eine weitere Möglichkeit ist das Prüfen auf einen möglichen Überlauf
wenn ein Häufigkeitszähler inkrementiert wird. Würde durch das
Inkrementieren eines Zählers ein Überlauf entstehen, werden alle
Häufigkeitszähler neu skaliert. Dafür werden zunächst die
Häufigkeitszähler aller Blätter des Huffman-Baums halbiert, was einer
einfachen Rechtsshift-Operation enstpricht. Dann werden von den
Blättern ausgehend bis zu Wurzel die Zähler aller Zwischenknoten mit
der Summe der Kinderknoten aktualisiert.

Da durch die Integer-Divison Genauigkeit verloren geht, kann es
passieren, dass der resultierende Baum kein Huffman-Baum mehr ist. Die
Lösung dafür ist die vollständige Neuerstellung des Huffman-Baums nach
jeder Skalierung. Aber selbst bei der Verwendung von 32-Bit Integern
sollte dies nicht sehr häufig passieren.

% Beispiel

Die Skalierung der Zähler hat eine Auswirkung auf die Komprimierung,
denn die Symbole, die nach der Skalierung gelesen werden, haben eine
größere Auswirkung auf die Häufigkeitszähler, als die Symbole vorher,
deren Wert halbiert wurde.  Dies kann sogar einen Vorteil haben, denn
die neu gelesenen Symbole können eine größere Abhängigkeit von den
kurz vohrer aufgetretenen Symbolen haben als von Symbolen, die weiter
in der Vergangenheit aufgetreten sind.

\subsection{Überlauf der Codeworte}
Wie in Abschnitt xy beschrieben, wird die maximale Länge eines
Codeworts von der Höhe des Huffman-Baums bestimmt.

% -> Salomon 5.2.7 Height of a Huffman Tree
% Betrifft nur den Kodierprozess

\subsection{Algorithmus von Vitter}

\section{Anwendungen}
In der Praxis wird die Huffman-Kodierung in der adaptiven Variante zum
Beispiel im Unix-Tool \emph{compact} verwendet. Bei vielen anderen
Programmen ist die Huffman-Kodierung Teil eines mehrstufigen Prozesses
der Komprimierung.

Die Huffman-Kodierung auch Teil des JPEG-Standards zur
verlustbehafteten Komprimierung von Bilder. Sie wurde aus Performance
und patentrechtlichen Gründen einer arithmetischen Kodierung
vorgezogen. \cite{Wallace:1991}

Das arithmetische Kodieren ist der größte Konkurrent der
Huffman-Kodierung. Der größte Vorteil über der Huffman-Kodierung ist
eine höhere Kompressionsrate bei den meisten Anwendungsfällen. Auf der
anderen Seite ist das arithmetische Kodieren deutlich aufwendiger und
somit langsamer als die Huffman-Kodierung. Auch ist das adaptive
arithmetische Kodieren nicht so robust wie die adaptive
Huffman-Kodierung, ein einzelner Fehler kann zum falschen Dekodieren
einer großen Datenmenge führen, während sich die adaptive
Huffman-Kodierung schnell wieder korrigiert. \cite{Bookstein:1993}


% JPG

% AAC

% MP3

% -> Salomon 5.2.9 Is Huffman Coding Dead

Die folgend vorgestellten Anwendungsfälle basieren auf \cite{Sayood:2006}.

% -> Sayood 3.8 Applications of Huffman Coding
\subsection{Textkomprimierung}

\subsection{Verlustfreie Bildkomprimierung}
Die Huffman-Kodierung kann zur verlustfreien Komprimierung von Bildern
verwendet werden.

Der einfachste Fall ist ein graustufiges Bild. Die Symbole des
Eingabealphabets wären in diesem Fall die Grauwerte, bei einer
8-Bit-Abstufung pro Pixel wären das die Werte von 0 bis 255.

Die Komprimierung mit der Huffman-Kodierung ist umso effektivsten, je
mehr gleiche Farb- bzw. Grautöne enthält das zu komprimierende Bild
enthält.

\subsection{Audiokomprimierung}

\renewcommand{\refname}{Literaturverzeichnis}
\bibliographystyle{unsrt}
\bibliography{adaptive_huffman}
\addcontentsline{toc}{section}{Literaturverzeichnis}

\end{document}
