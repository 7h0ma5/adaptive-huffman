\documentclass[twoside,11pt,a4paper]{article}

\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{latexsym,amsmath,amssymb}
\usepackage[ngerman]{babel}
\usepackage{theorem}
\usepackage{dcolumn}
\usepackage{url}
\usepackage{tikz}
\usepackage{caption}
\usepackage{subcaption}
\usepackage[hidelinks]{hyperref}
\usetikzlibrary{trees,calc}

\newcommand{\Frac}[2]{\frac{\displaystyle #1}{\displaystyle #2}}
\newlength{\textwd}
\newlength{\oddsidemargintmp}
\newlength{\evensidemargintmp}
\newcommand{\hspaceof}[2]{\settowidth{\textwd}{#1}\mbox{\hspace{#2\textwd}}}
\newlength{\textht}
\newcommand{\vspaceof}[3]{\settoheight{\textht}{#1}\mbox{\raisebox{#2\textht}{#3}}}
\newcommand{\PreserveBackslash}[1]{\let\temp=\\#1\let\\=\temp}

\newenvironment{deflist}[1][\quad]%
{  \begin{list}{}{%
      \renewcommand{\makelabel}[1]{\textbf{##1}\hfil}%
      \settowidth{\labelwidth}{\textbf{#1}}%
      \setlength{\leftmargin}{\labelwidth}
      \addtolength{\leftmargin}{\labelsep}}}
{  \end{list}}

\newenvironment{Quote}% Definition of Quote
{  \begin{list}{}{%
      \setlength{\rightmargin}{0pt}}
      \item[]\ignorespaces}
{\unskip\end{list}}

\newtheorem{Cor}{Corollary}
\theoremstyle{break}
\theorembodyfont{\itshape}
\newtheorem{Def}[Cor]{Definition}
\theoremheaderfont{\scshape}

\pagestyle{headings}

\textwidth 15cm
\textheight 22.5cm
\oddsidemargin 1cm
\evensidemargin 0cm

\tikzset{
grow=down,
level 1/.style={sibling distance=2.5cm, level distance=1.3cm},
level 2/.style={sibling distance=2.0cm, level distance=1.3cm},
level 3/.style={sibling distance=1.5cm, level distance=1.3cm},
level 4/.style={sibling distance=1.0cm, level distance=1.3cm},
edge from parent path={
(\tikzparentnode) |-
($(\tikzparentnode)!0.5!(\tikzchildnode)$) -|
(\tikzchildnode)}
}

\tikzstyle{end} = [circle, minimum width=5pt, fill, inner sep=0pt]

\renewcommand*\ttdefault{txtt}

\usepackage{parskip}
%\setlength{\parindent}{0pt}
%\setlength{\parskip}{2ex plus 0.5ex minus 0.2ex}

%\setlength{\parindent}{0pt}
%\setlength{\parskip}{1ex plus 0.5ex minus 0.2ex}

\usepackage{titlesec}
\titlespacing\section{0pt}{12pt plus 4pt minus 2pt}{0pt plus 2pt minus 2pt}
\titlespacing\subsection{0pt}{12pt plus 4pt minus 2pt}{0pt plus 2pt minus 2pt}
\titlespacing\subsubsection{0pt}{12pt plus 4pt minus 2pt}{0pt plus 2pt minus 2pt}

\begin{document}
%\raggedbottom

\pagestyle{empty}
\begin{center}
    Rheinisch-Westfälische Technische Hochschule Aachen \\
    Lehrstuhl für Informatik VI \\
    Prof. Dr.-Ing. Hermann Ney\\[6ex]
    Proseminar Datenkompression im WS 2014/2015\\[12ex]

    \LARGE
    \textbf{Adaptive Huffman-Kodierung und Anwendungen} \\[6ex]
    \textit{Thomas Gatzweiler} \\[6ex]
    \Large
    Matrikelnummer 318947 \\[6ex]
    12. November 2014

    \vfill
    \Large Betreuer: Patrick Dötsch
\end{center}

\newpage
\
\newpage

\pagestyle{headings}
\tableofcontents
\listoftables
\listoffigures
\newpage
\pagestyle{empty}
\newpage
\pagestyle{headings}


\newcommand{\sectionbreak}{\clearpage}

% Literaturverzeichnis wie in der .bib Datei ordnen
\nocite{*}

\section{Einleitung}
Auch wenn heute die verfügbaren Speicherkapazitäten und Bandbreiten
größer als jemals zuvor sind, ist die Komprimierung von Daten immer
noch ein wichtiges Thema. Mit der Datenkompression lassen sich
Übertragungskanäle noch weiter optimieren und Datenspeicher noch
besser ausnutzen.

Es gibt eine Vielzahl von Methoden zur Datenkompression, viele der
Methoden sind für spezielle Anwendungsfälle entwickelt worden. Eine
Form der Datenkompression ist die Entropiekodierung. Sie komprimiert
Daten, indem Bitfolgen aus den Eingabedaten, die sogenannten Symbole,
mit anderen Bitfolgen, den sogenannten Codewörtern, ersetzt
werden. Die Komprimierung wird erreicht, indem häufiger vorkommende
Symbole mit kürzeren Codewörtern ersetzt und seltener vorkommende
Symbole mit längeren Codewörtern ersetzt werden.

Ein Symbol kann ein eine einfache Bitfolge, ein Buchstabe, ein Wort
oder auch eine Kombination von Wörtern sein. Wie ein Symbol definiert
ist, ist vom Anwendungsfall abhängig. Es kann beispielsweise mehr Sinn
machen, bei einem Text die Wörter als Symbole zu definieren, als die
Buchstaben. Die Menge an Symbolen, die in einer Datenmenge existieren,
wird als Alphabet bezeichnet.

Diese Ausarbeitung befasst sich mit der Huffman-Kodierung, eine Form
der Entropiekodierung, der adaptiven Huffman-Kodierung und den
Anwendungen der Huffman-Kodierung, welche in der Praxis meist in der
adaptiven Variante verwendet wird.

\section{Huffman-Kodierung} \label{sec:huffman}
Dieser Abschnitt ist eine Einführung in die Huffman-Kodierung, welche
die Grundlage der adaptive Huffman-Kodierung ist. Der Abschnitt
basiert auf \cite[S. 211 -- 221]{Salomon:2010} und \cite[S. 41 --
  45]{Sayood:2006}.

Die Huffman-Kodierung ist eine Entropiekodierung, das heißt, sie
kodiert Daten basierend auf der Häufigkeit der vorkommenden Symbole
des Eingabealphabets. Häufig vorkommende Symbole werden mit kürzeren
Codewörtern kodiert als seltener vorkommende Symbole. Die
Huffman-Kodierung wurde 1952 von David Huffman entwickelt und ist
seitdem Gegenstand vieler Forschungsarbeiten.

Es gibt Fälle, in denen durch die Huffman-Kodierung keine
Komprimierung erzielt werden kann, zum Beispiel bei Symbolen mit
gleicher Häufigkeit, da es sich dabei stochastisch gesehen um
Zufallsdaten handelt. Beispielsweise ist eine Zeichenkette
$a_1a_1{\dots}a_1a_2a_2{\dots}a_2a_3a_3{\dots}a_3$ mit der
Huffman-Kodierung nicht komprimierbar. Bei einem Alphabet, welches nur
aus zwei Symbolen besteht, kann die Huffman-Kodierung auch nicht zur
Komprimierung verwendet werden. Einem Symbol würde in diesem Fall
entweder die {\tt1} oder die {\tt0} als Codewort zugewiesen werden.
Da die Huffman-Kodierung keine Codewörter vergeben kann, die kürzer
als ein Bit sind, ist es nicht möglich, dieses Alphabet zu verbessern.

Der Codewörter, die bei der Huffman-Kodierung erzeugt werden, sind
immer präfixfrei, das heißt kein Codewort ist Präfix eines anderen
Codeworts. Das macht es möglich, die erzeugten Codewörter ohne
Trennzeichen direkt hintereinander zu schreiben und beim Dekodieren
eindeutig zu erkennen.

Die Huffman-Kodierung basiert auf zwei Beobachtungen über optimale
präfixfreie Codes:

\begin{enumerate}
\item In einem optimalen Code haben die Symbole, die häufiger
  auftreten, also eine höhere Auftrittswahrscheinlichkeit haben,
  kürzere Codewörter als Symbole, die seltener auftreten.
\item In einem optimalen Code haben die Codewörter für die beiden
  Symbole, die am seltensten auftreten, die selbe Länge.
\end{enumerate}

Die erste Beobachtung ist offensichtlich korrekt. Wäre die Länge des
Codeworts eines seltener auftretenden Symbols kürzer als die eines
häufiger auftretenden Symbols, dann wäre die mittlere Länge eines
Codeworts länger, weshalb ein solcher Code nicht optimal sein kann.

Die zweite Beobachtung lässt sich einfach zeigen. Angenommen es gäbe
einen optimalen Code, bei dem die zwei am seltensten auftretenden
Symbole Codewörter mit unterschiedlicher Länge mit dem
Längenunterschied $k$ zugewiesen bekommen. Weil es sich um einen
präfixfreien Code handelt, kann das kürzere Codewort kein Präfix des
längeren Codeworts sein. Das bedeutet man kann die letzten $k$ Bits
des längeren Codeworts wegfallen lassen und die beiden Codewörter
wären immer noch eindeutig. Da es sich bei den Symbolen der zwei
Codewörter um die am seltensten Symbole eines Alphabets handelt, gibt
es keine längeren Codewörter. Aus diesem Grund ist es nicht möglich,
dass das gekürzte Codewort Präfix eines längeren Codeworts
wird. Außerdem entsteht durch das Entfernen dieser $k$ Bits ein neuer
Code, der eine kürzere mittlere Codewortlänge hat und somit besser ist
als der Ausgangscode. Das ist ein Widerspruch zu der Annahme, dass es
sich bei dem Ausgangscode um einen optimalen Code handelt. Die zweite
Feststellung trifft somit auch zu.

Die Huffman-Kodierung erweitert diese Beobachtungen um die Bedingung,
dass sich die Codewörter der zwei seltensten Symbole nur beim letzten
Bit unterscheiden dürfen.

\subsection{Huffman-Baum} \label{sec:HTREE}
Um den Symbolen des Eingabealphabets Codewörter zuzuweisen, wird der
sogenannte \emph{Huffman-Baum} aufgebaut. Es handelt sich dabei um
einen Binärbaum, damit die entstehenden Codewörter binär sind.

Um den Huffman-Baum aufzubauen muss die Wahrscheinlichkeitsverteilung
für die Symbole des Eingabealphabets bekannt sein. Die
Wahrscheinlichkeiten können entweder geschätzt werden, wie zum
Beispiel mit den Buchstabenhäufigkeiten einer Sprache, oder sie müssen
in einem zusätzlichen Durchgang bestimmt werden.

Als nächstes wird die Liste der Symbole nach Häufigkeiten geordnet.
Dann Huffman-Baum wird von unten nach oben aufgebaut, also von den
Blättern zur Wurzel. Es werden immer die beiden Symbole mit der
niedrigsten Auftrittswahrscheinlichkeit aus der Liste entfernt und ein
Knoten erstellt, der diese beiden Symbole als Nachfolger hat. Dieser
Knoten wird dann als Hilfssymbol mit der summierten Wahrscheinlichkeit
wieder in der Liste eingeordnet. Der Huffman-Baum ist fertig, wenn die
Liste auf ein Hilfssymbol reduziert ist.

Beim Erstellen des Huffman-Baums gibt verschiedene Möglichkeiten, wenn
es Symbole mit den gleichen Wahrscheinlichkeiten gibt. Die Wahl einer
dieser Möglichkeiten hat keinen Einfluss auf die mittlere Länge der
Codewörter, nur auf Zusammensetzung der erzeugten Codewörter.

\begin{table}[h]
\centering
\caption{Beispiel für eine Häufigkeitsverteilung}

\begin{subtable}[t]{0.3\textwidth}
  \centering
  \caption{Ausgangstabelle}
  \begin{tabular}{c|r|r}
    $a_i$ & $H(a_i)$ & $P(a_i)$ \\ \hline
    A & 40   & 0.20 \\
    B & 4    & 0.02 \\
    C & 16   & 0.08 \\
    D & 20   & 0.10 \\
    E & 120  & 0.60 \\
  \end{tabular}
  \vspace{3ex}
  \label{tab:HBEX0}
\end{subtable}
\begin{subtable}[t]{0.3\textwidth}
  \centering
  \caption{Schritt 1}
  \begin{tabular}{c|c}
    $a_i$ & $P(a_i)$ \\ \hline
    E & 0.60 \\
    A & 0.20 \\
    D & 0.10 \\
    C & 0.08 \\
    B & 0.02 \\
  \end{tabular}
  \vspace{3ex}
  \label{tab:HBEX1}
\end{subtable}
\begin{subtable}[t]{0.3\textwidth}
  \centering
  \caption{Schritt 2}
  \begin{tabular}{c|c}
    $a_i$ & $P(a_i)$ \\ \hline
    E & 0.60 \\
    A & 0.20 \\
    D & 0.10 \\
    \{C, B\} & 0.10 \\ \\
  \end{tabular}
  \vspace{3ex}
  \label{tab:HBEX2}
\end{subtable}
\begin{subtable}[t]{0.3\textwidth}
  \centering
  \caption{Schritt 3}
   \begin{tabular}{c|c}
    $a_i$ & $P(a_i)$ \\ \hline
    E & 0.60 \\
    A & 0.20 \\
    \{B, C, D\} & 0.20 \\
  \end{tabular}
  \label{tab:HBEX3}
\end{subtable}
\begin{subtable}[t]{0.3\textwidth}
  \centering
  \caption{Schritt 4}
   \begin{tabular}{c|c}
    $a_i$ & $P(a_i)$ \\ \hline
    E & 0.60 \\
    \{A, B, C, D\} & 0.40 \\ \\
  \end{tabular}
  \label{tab:HBEX4}
\end{subtable}
\begin{subtable}[t]{0.3\textwidth}
  \centering
  \caption{Schritt 5}
  \begin{tabular}{c|c}
    $a_i$ & $P(a_i)$ \\ \hline
    \{A, B, C, D, E\} & 1.00 \\ \\ \\
  \end{tabular}
  \label{tab:HBEX5}
\end{subtable}
\label{tab:HBEX}
\end{table}

Am besten lässt sich die Erzeugung eines Huffman-Baums mit Hilfe eines
Beispiels verstehen. In Tabelle~\ref{tab:HBEX0} ist ein Beispiel einer
Häufigkeitsverteilung dargestellt, mit den Symbolen $\{a_i \mid i \in
[1, 5]\}$, den absoluten Häufigkeiten der Symbole, $H(a_i)$, und die
Wahrscheinlichkeiten der Symbole, $P(a_i)$.

Nachdem alle Symbole nach den Wahrscheinlichkeiten geordnet sind,
ergibt sich Tabelle~\ref{tab:HBEX1}. Die am seltensten Vorkommenden
Symbole sind $C$ und $B$, also wird für $C$ und $B$ jeweils ein
Blattknoten erzeugt. Dann wird ein Elternknoten $\{C, B\}$ erzeugt,
der die summierte Wahrscheinlichkeit beider Symbole von $0.10$
besitzt. Für diesen Knoten wird das Hilfssymbol $\{C, B\}$ zurück in
die Liste geschrieben und es ergibt sich Tabelle~\ref{tab:HBEX2}.

Nun sind die seltensten Symbole in der Liste das Hilfssymbol $\{C,
B\}$ und das Symbol $D$. Für $D$ wird wieder ein Blattknoten erzeugt,
für $\{C, B\}$ existiert bereits ein Knoten. Dann wird ein
Elternknoten $\{B, C, D\}$ erzeugt, der als Nachfolger den Blattknoten
für $D$ und den Knoten $\{C, B\}$ hat. Dieser wird als Hilfssymbol
$\{B, C, D\}$ mit der summierten Wahrscheinlichkeit von $0.20$ in die
Liste geschrieben und es ergibt sich Tabelle~\ref{tab:HBEX3}.

Der gleiche Prozess wird noch zwei mal wiederholt (siehe
Tabelle~\ref{tab:HBEX4} und \ref{tab:HBEX5}), danach existiert nur
noch ein Hilfssymbol in der Liste. Der Huffman-Baum ist nun fertig
und in Abbildung~\ref{fig:HBEX} dargestellt.

\begin{figure}[h]
\centering
\begin{tikzpicture}
\node {\{A, B, C, D, E\}}
child {
  node {E}
  edge from parent
  node[left] {\scriptsize $0.60$}
}
child {
  node {\{A, B, C, D\}}
  child {
    node {A}
    edge from parent
    node[left] {\scriptsize $0.20$}
  }
  child {
    node {\{B, C, D\}}
    child {
      node {D}
      edge from parent
      node[left] {\scriptsize $0.10$}
    }
    child {
      node {\{C, B\}}
      child {
        node {C}
        edge from parent
        node[left] {\scriptsize $0.08$}
      }
      child {
        node {B}
        edge from parent
        node[right] {\scriptsize $0.02$}
      }
      edge from parent
      node[right] {\scriptsize $0.10$}
    }
    edge from parent
    node[right] {\scriptsize $0.20$}
  }
  edge from parent
  node[right] {\scriptsize $0.40$}
};
\end{tikzpicture}
\caption{Huffman-Baum für die Verteilung aus Tabelle~\ref{tab:HBEX}} \label{fig:HBEX}
\end{figure}

\subsection{Kodierung}
Nachdem der Huffman-Baum erzeugt wurde, kann er verwendet werden, um
die Codewörter für die Symbole zu bestimmen. Aus dem Pfad von der
Wurzel zu einem Blatt lässt sich das Codewort für dieses Blatt
ableiten, indem linke Abzweige von einem Knoten als ein Bit und rechte
Abzweige als das andere Bit interpretiert werden. In der Praxis läuft
der Kodierer bedingt von der Datenstruktur, in der der Baum gespeichert
ist, vom Knoten zur Wurzel und muss das erhaltene Codewort wieder
umdrehen.

In Abbildung~\ref{fig:HKOD} ist ein Huffman-Baum mit den Symbolen
\emph{H}, \emph{R}, \emph{T} und \emph{W} abgebildet. Um das Wort
\emph{RWTH} zu kodieren, wird nun für jedes Symbol der Pfad von dem
Blatt dieses Symbols zur Wurzel durchschritten. Dabei wird das
Codewort gebildet, indem bei jedem Knoten, der auf dem Weg erreicht
wird, eine {\tt1} an das Codewort gehängt, wenn der Knoten der linke
Nachfolger das Elternknoten ist, und eine {\tt0} angehängt, wenn der
Knoten der rechte Nachfolger des Elternknotens ist. Das auf diesem
Weg erhaltene Codewort wird dann noch umgedreht.

Für \emph{R} erhält man so das Codewort {\tt000}, für \emph{W} das
Codewort {\tt001}, für \emph{T} das Codewort {\tt1} und für \emph{H}
das Codewort \emph{\tt01}. Insgesamt wird \emph{RWTH} also zu
{\tt000001101} kodiert.

\begin{figure}[h]
\centering
\begin{tikzpicture}
\node {}
    child {
        node {\tt1}
        child {
            node {T}
            edge from parent
        }
        edge from parent
        node[left]  {$\frac{11}{20}$}
    }
    child {
        node {\tt0}
        child {
                node {\tt1}
                child {
                    node {H}
                    edge from parent
                }
                edge from parent
                node[left] {$\frac{5}{20}$}
            }
            child {
                node {\tt0}
                child {
                    node {\tt1}
                    child {
                        node {W}
                        edge from parent
                    }
                    edge from parent
                    node[left] {$\frac{3}{20}$}
                }
                child {
                    node {\tt0}
                    child {
                        node {R}
                        edge from parent
                    }
                    edge from parent
                    node[right] {$\frac{1}{20}$}
                }
                edge from parent
                node[right] {$\frac{4}{20}$}
            }
        edge from parent
        node[right] {$\frac{9}{20}$}
    };
\end{tikzpicture}
\caption{Kodierung mit Hilfe eines Huffman-Baums} \label{fig:HKOD}
\end{figure}

\subsection{Dekodierung} \label{sec:huffman-dec}
Die Dekodierung kann mit Hilfe des Huffman-Baums durchgeführt
werden. Damit der Dekodierer einen Huffman-Baum erzeugen kann, muss
entweder die Häufigkeitsverteilung oder der Huffman-Baum zusammen mit
den kodierten Daten gespeichert bzw. übertragen werden. Wenn dem
Dekodierer nur die Häufigkeitsverteilung bekannt ist, muss er den
Huffman-Baum nach exakt dem gleichen Prinzip aufbauen, wie der
Kodierer, damit der selbe Huffman-Baum entsteht und die Daten korrekt
dekodiert werden können.

Die kodierten Daten werden ähnlich wie beim Kodieren durch das
traversieren des Huffman-Baums dekodiert. Der Dekodierer startet bei
der Wurzel und geht für jedes gelesene Bit entweder zum linken oder
rechten Nachfolger des aktuellen Knotens. Sobald ein Blatt des Baums
erreicht wird, ist ein Symbol fertig dekodiert, der Dekodierer gibt
das im Blatt gespeicherte Symbol aus und kehrt zurück zur Wurzel des
Baums und verarbeitet das nächste Bit.

\section{Adaptive Huffman-Kodierung}
Dieser Abschnitt beschreibt die Funktionsweise der adaptiven Huffman-Kodierung. Er basiert auf \cite[S. 234 --
 S. 238]{Salomon:2010}.

Für die Huffman-Kodierung ist das Wissen über die
Wahrscheinlichkeitsverteilung notwendig der zu kodierenden Daten
notwendig, um die Codewörter für die Symbole des Eingabealphabets zu
bestimmen. Es gibt jedoch Fälle, in denen diese Verteilung im voraus
nicht bekannt ist, zum Beispiel bei der Übertragung von Echtzeitdaten
über ein Netzwerk. Auch ist Verfahren zur Kodierung einer Datei in nur
einem Durchgang wünschenswert. Normalerweise muss eine Datei zunächst
komplett gelesen werden, um die Wahrscheinlichkeiten der vorkommenden
Symbole zu bestimmen, aber besonders bei großen Datenmengen und
begrenztem Arbeitsspeicher ist dies ein erheblicher Rechenaufwand.

Die Methode zur Lösung dieser Problemstellung ist die \emph{adaptive
  Huffman-Kodierung}. Mit ihr lassen sich Datenströme in einem
Durchgang und ohne Verzögerung kodieren, für jedes Eingangssymbol wird
sofort ein Ausgangssymbol erzeugt. Sie ist basiert auf der
Huffman-Kodierung, passt sich aber laufend an die
Wahrscheinlichkeitsverteilung des Eingabealphabets an. Die adaptive
Huffman-Kodierung wurde ursprünglich 1973 von Newton Faller und 1978
Robert Gallager unabhängig entwickelt und 1985 von Donald Knuth
wesentlich verbessert.

Auch die adaptive Huffman-Kodierung verwendet einen Huffman-Baum zur
Erzeugung der Codeworte. Da anfangs jedoch noch keine Häufigkeiten der
auftretenden Symbole und auch die Symbole selbst nicht bekannt sind,
beginnen Kodierer und Dekodierer mit einem leeren Baum. Von diesem
Punkt an aktualisieren Kodierer und Dekodierer den Baum mit jeden
neuen Wort entsprechend der neuen Häufigkeitsverteilung. Jeder
Blattknoten besitzt dafür ein Integer-Feld, in dem die absolute
Häufigkeit des zugehörigen Symbols gespeichert ist. Wenn es sich nicht
um einen Blattknoten handelt, wird die Summe der Häufigkeiten beider
Nachfolgeknoten gespeichert. Beim Inkrementieren des
Häufigkeitszählers eines Blattknotens müssen deshalb auch alle
Elternknoten bis zu Wurzel aktualisiert werden. Es wäre auch
möglich die Wahrscheinlichkeiten als Fließkommazahlen zu speichern,
dies wäre jedoch ein höherer Rechenaufwand und kann wegen mangelnder
Präzision zu Problemen führen. Aber auch die Speicherung als Integer bringt
Probleme mit sich, näheres dazu in Abschnitt~\ref{sec:AHCOUNTER}.

Es ist wichtig, dass Kodierer und Dekodierer die auf den Huffman-Baum
angewandten Operationen exakt spiegeln und mit Mehrdeutigkeiten auf
die selbe Weise umgehen.

\subsection{Kodierung}
Der Kodierer beginnt das Lesen der Eingabedaten mit einem leeren
Baum. Das erste gelesene Symbol gibt der Kodierer unkomprimiert aus
und fügt das Symbol dann in den Baum ein, wodurch ab diesem Punkt ein
Codewort für dieses Symbol existiert. Wenn der Kodierer das nächste
mal auf dieses Symbol trifft, wird dieses Codewort ausgegeben und der
Häufigkeitszähler für dieses Symbol im Huffman-Baum inkrementiert.
Dabei kann es passieren, dass der Huffman-Baum nicht mehr das
Kriterium eines optimalen Codes erfüllt (siehe
Abschnitt~\ref{sec:huffman}).  Deshalb muss nach jedem Symbol
überprüft werden, ob dies der Fall ist, falls ja muss der Huffman-Baum
neu angeordnet werden, wodurch sich die Codewörter ändern.

\subsection{Dekodierung}
Der Dekodierer vollzieht exakt die selben Schritte. Wenn er ein
unkomprimiertes Symbol liest, fügt er dies zu seinem Huffman-Baum
hinzu und hat so für das nächste Auftreten dieses Symbols das selbe
Codewort wie der Kodierer. Wenn er ein kodiertes Symbol -- also ein
Codewort -- liest, wird es, wie in Abschnitt~\ref{sec:huffman-dec}
beschrieben, dekodiert und ausgegeben. Zusätzlich wird der
Häufigkeitszähler für dieses Symbol inkrementiert und der Huffman-Baum
gegebenenfalls wie beim Kodierer neu angeordnet.

Nun stellt sich das Problem, dass der Dekodierer nicht zwischen
unkomprimierten Codewörtern unterscheiden kann, da es sich im beiden
Fällen nur um eine Bitfolge im Datenstrom handelt. Um diese Unklarheit
zu beseitigen wird jedes unkomprimierte Symbol mit einem speziell
reservierten Codewort begonnen, dem sogenannten \emph{Escape-Code}.
Nach dem Escape-Code kann dann zum Beispiel ein einzelnes
ASCII-Zeichen folgen oder auch längere Bitfolgen, die entweder mit der
zu erwartenden Länge beginnen oder selbst mit einem eigenen
eindeutigen Escape-Code enden.

Weiterhin besteht das Problem, dass der Escape-Code kein Codewort
eines Symbols sein darf und auch kein Präfix davon. Erschwerend kommt
hinzu, dass sich die Codewörter durch die Neuanordnung des
Huffman-Baums laufend verändern können. Der natürlichste Weg ist also,
ein eigenes, leeres Blatt für den Escape-Code in den Huffman-Baum
einzufügen. Das bringt den Vorteil mit sich, dass sich die Länge des
Codeworts für den Escape-Code automatisch an die Häufigkeit des
Auftretens anpasst. Je seltener ein zuvor unbekanntes Symbol auftritt,
welches unkomprimiert übertragen werden muss, desto länger ist das
Codewort für den Escape-Code im Vergleich zu den Codewörtern der
anderen Symbole. Abbildung~\ref{fig:HESC} zeigt einen Huffman-Baum mit
einem Platzhalter für den Escape-Code, der momentan das Codewort
{\tt001} besitzt.

\begin{figure}[h]
\centering
\begin{tikzpicture}
\node {}
child {
  node {A}
  edge from parent
  node[left]  {\tt1}
}
child {
  child {
    node {B}
    edge from parent
    node[left] {\tt1}
  }
  child {
    child {
      node[end] {}
      edge from parent
      node[left] {\tt1}
    }
    child {
      node {C}
      edge from parent
      node[right] {\tt0}
    }
    edge from parent
    node[right] {\tt0}
  }
  edge from parent
  node[right] {\tt0}
};
\end{tikzpicture}
\caption{Ein Huffman-Baum mit Platzhalter für den Escape-Code} \label{fig:HESC}
\end{figure}

\subsection{Aktualisierung des Huffman-Baums}
Der Huffman-Baum muss sowohl im Kodierer als auch im Dekodierer nach
jeder Aktualisierung eines Häufigkeitszählers überprüft werden und
dann, falls es sich nicht länger um einen Huffman-Baum handelt,
aktualisiert werden. Um dies zu Prüfen, muss der gesamte Huffman-Baum
von rechts unten nach links oben Level für Level gescannt
werden. Dabei müssen sich die Häufigkeitszähler in einer geordneten
Reihenfolge befinden und kein Häufigkeitszähler darf kleiner als der
Vorherige sein. Diese Eigenschaft wird auch
\emph{Geschwister-Eigenschaft} (sibling property) genannt.

\begin{figure}[h]
\centering
\begin{tikzpicture}[
  level 1/.style={sibling distance=3.0cm, level distance=1.5cm},
  level 2/.style={sibling distance=2.5cm, level distance=1.5cm},
  level 3/.style={sibling distance=1.0cm, level distance=1.5cm}
]
\node {70}
child {
  node{A}
  edge from parent
  node[left] {39}
}
child {
  child {
    child {
      node {B}
      edge from parent
      node[left] {8}
    }
    child {
      node {C}
      edge from parent
      node[right] {8}
    }
    edge from parent
    node[left] {16}
  }
  child {
    child {
      node {D}
      edge from parent
      node[left] {8}
    }
    child {
      node {E}
      edge from parent
      node[right] {7}
    }
    edge from parent
    node[right] {15}
  }
  edge from parent
  node[right] {31}
};
\end{tikzpicture}
\caption{Ein korrekter Huffman-Baum} \label{fig:HCORR}
\end{figure}

In Abbildung~\ref{fig:HCORR} ist ein korrekter Huffman-Baum
dargestellt. Der Knoten unten rechts besitzt mit 7 den kleinsten
Häufigkeitszähler, die Wurzel mit 70 den Größten. Geht man die Knoten
von unten rechts nach oben links durch, erhält man die Häufigkeiten 7,
8, 8, 8, 15, 16, 31, 39 und 70. Diese Folge ist monoton steigend, die
Bedingung für einen Huffman-Baum trifft für diesen Baum also zu.

Es gibt einen einfachen rekursiven Algorithmus mit dem der
Huffman-Baum aktualisiert werden kann. Er wird mit dem Blattknoten
$X$, dessen Häufigkeitszähler $F$ inkrementiert werden soll,
aufgerufen. Der Algorithmus besteht aus den folgenden drei Schritten:

\begin{enumerate}
\item Vergleiche $X$ mit den Nachfolgern im Baum (von rechts nach
  links und von unten nach oben). Sollte der direkte Nachfolger von
  $X$ einen Häufigkeitszähler von $F+1$ oder mehr haben, sind die
  Knoten noch in der richtigen Reihenfolge und es gibt nichts zu tun.
  Falls dies nicht der Fall ist, existieren ein oder mehrere
  Nachfolger von $X$, deren Häufigkeitszähler gleich oder kleiner als
  $F$ sind. In diesem Fall muss $X$ mit dem letzten dieser Nachfolger
  getauscht werden. Die einzige Ausnahme ist, dass $X$ nicht mit dem
  Elternknoten getauscht werden darf.
\item Inkrementiere den Häufigkeitszähler des Knotens $X$ von $F$ auf
  $F+1$.
\item Wenn $X$ der Wurzelknoten ist, stoppe die Ausführung, sonst
  führe den Algorithmus auf dem Elternknoten von $X$ aus.
\end{enumerate}

Abbildung~\ref{fig:HUPD} zeigt zwei Beispiele der Aktualisierung eines
Huffman-Baums. Das erste Beispiel geht vom Huffman-Baum in
Abbildung~\ref{fig:HCORR} aus und inkrementiert den Häufigkeitszähler
für das Symbol $E$. Das Ergebnis der Aktualisierung sieht man in
Abbildung~\ref{fig:HUPD1}: Der Häufigkeitszähler von $E$ steht nun auf
8 und auch die Häufigkeiten der Elternknoten wurden jeweils
inkrementiert.

Im zweiten Beispiel wird $E$ erneut inkrementiert. Nun wird im
1. Schritt des Aktualisierungs-Algorithmus festgestellt, dass der
Knoten von $B$ beim Durchgehen von unten links nach oben rechts der
letzte Knoten ist, der einen Häufigkeitszähler kleiner-gleich dem von
$E$ ist. Also werden die beiden Knoten getauscht und der
Häufigkeitszähler von $E$ erhöht. Beim Inkrementieren der Zähler der
Elternknoten treten in diesem Fall keine weiteren Probleme auf. Das
Ergebnis der Operation ist in Abbildung~\ref{fig:HUPD2} dargstellt.

\begin{figure}[h]
\centering
\begin{subfigure}[t]{0.45\textwidth}
\begin{tikzpicture}[
  level 1/.style={sibling distance=3.0cm, level distance=1.5cm},
  level 2/.style={sibling distance=2.5cm, level distance=1.5cm},
  level 3/.style={sibling distance=1.0cm, level distance=1.5cm}
]
\node {71}
child {
  node{A}
  edge from parent
  node[left] {39}
}
child {
  child {
    child {
      node {B}
      edge from parent
      node[left] {8}
    }
    child {
      node {C}
      edge from parent
      node[right] {8}
    }
    edge from parent
    node[left] {16}
  }
  child {
    child {
      node {D}
      edge from parent
      node[left] {8}
    }
    child {
      node {E}
      edge from parent
      node[right] {8}
    }
    edge from parent
    node[right] {16}
  }
  edge from parent
  node[right] {32}
};
\end{tikzpicture}
\vspace{2ex}
\caption{1. Inkrementierung von $E$} \label{fig:HUPD1}
\end{subfigure}
\begin{subfigure}[t]{0.45\textwidth}
\begin{tikzpicture}[
  level 1/.style={sibling distance=3.0cm, level distance=1.5cm},
  level 2/.style={sibling distance=2.5cm, level distance=1.5cm},
  level 3/.style={sibling distance=1.0cm, level distance=1.5cm}
]
\node {72}
child {
  node{A}
  edge from parent
  node[left] {39}
}
child {
  child {
    child {
      node(X) {E}
      edge from parent
      node[left] {9}
    }
    child {
      node {C}
      edge from parent
      node[right] {8}
    }
    edge from parent
    node[left] {17}
  }
  child {
    child {
      node {D}
      edge from parent
      node[left] {8}
    }
    child {
      node(Y) {B}
      edge from parent
      node[right] {8}
    }
    edge from parent
    node[right] {16}
  }
  edge from parent
  node[right] {33}
};
\draw[<->, dashed] (Y.north west) to[out=130,in=50,looseness=1.5] (X.north east);
\end{tikzpicture}
\vspace{2ex}
\caption{2. Inkrementierung von $E$} \label{fig:HUPD2}
\end{subfigure}
\vspace{2ex}
\caption{Aktualisierung eines Huffman-Baums} \label{fig:HUPD}
\end{figure}


\subsection{Überlauf der Häufigkeitszähler} \label{sec:AHCOUNTER}
Die Häufigkeitszähler für das Auftrittshäufigkeit von Eingabesymbolen
werden bei der adaptiven Huffman-Kodierung als Integer
gespeichert. Abhängig von der Datenmenge kann es passieren, dass die
auftretende Häufigkeit eines Symbols die Grenze des darstellbaren
Zahlenbereichs überschreitet -- es kommt zu einem Integerüberlauf.

Der darstellbare Zahlenbereich für Integer ist abhängig von der
verwendeten Hardware. Mit modernen 64-Bit Rechnerarchitekturen ist die
Wahrscheinlichkeit für einen Überlauf heute deutlich geringer, die
Möglichkeit sollte dennoch nicht vernachlässigt werden. Viele
Mikrocontroller rechnen standardmäßig mit 8- oder 16-Bit-Integern, die
nur 256 bzw. 65536 Werte darstellen können.

Eine Möglichkeit zu Verhinderung der Überläufe wäre die Benutzung
einer Bibliothek zur Speicherung von Integern mit dynamischer Größe,
wie zum Beispiel die \emph{GNU Multiple Precision Arithmetic
  Library}. Ein Nachteil ist der entstehende Overhead, außerdem ist
die Verwendung einer solchen Bibliothek bei vielen Mikrocontrollern
nicht möglich. \cite{GMP}

Eine weitere Möglichkeit ist das Prüfen auf einen möglichen Überlauf
wenn ein Häufigkeitszähler inkrementiert wird. Würde durch das
Inkrementieren eines Zählers ein Überlauf entstehen, werden alle
Häufigkeitszähler neu skaliert. Dafür werden zunächst die
Häufigkeitszähler aller Blätter des Huffman-Baums halbiert, was einer
einfachen Rechtsshift-Operation entpricht. Dann werden von den
Blättern ausgehend bis zu Wurzel die Zähler aller Zwischenknoten mit
der Summe der Nachfolgeknoten aktualisiert.

\begin{figure}[h]
\centering
\begin{subfigure}[t]{0.45\textwidth}
\centering
\begin{tikzpicture}[
  level 1/.style={sibling distance=3.3cm, level distance=1.5cm},
  level 2/.style={sibling distance=1.4cm, level distance=1.5cm},
]
\node {930}
child {
  child {
    node {A}
    edge from parent
    node[left] {310}
  }
  child {
    node {B}
    edge from parent
    node[right] {310}
  }
  edge from parent
  node[left] {620}
}
child {
  child {
    node {C}
    edge from parent
    node[left] {155}
  }
  child {
    node {D}
    edge from parent
    node[right] {155}
  }
  edge from parent
  node[right] {310}
};
\end{tikzpicture}

\vspace{2ex}
\caption{Vor der Skalierung} \label{fig:HSCAL1}
\end{subfigure}
\begin{subfigure}[t]{0.45\textwidth}
\centering
\begin{tikzpicture}[
  level 1/.style={sibling distance=3.3cm, level distance=1.5cm},
  level 2/.style={sibling distance=1.4cm, level distance=1.5cm},
]
\node {464}
child {
  child {
    node {A}
    edge from parent
    node[left] {155}
  }
  child {
    node {B}
    edge from parent
    node[right] {155}
  }
  edge from parent
  node[left] {310}
}
child {
  child {
    node {C}
    edge from parent
    node[left] {77}
  }
  child {
    node {D}
    edge from parent
    node[right] {77}
  }
  edge from parent
  node[right,red] {\textbf{154}}
};
\end{tikzpicture}

\vspace{2ex}
\caption{Nach der Skalierung} \label{fig:HSCAL2}
\end{subfigure}
\vspace{2ex}
\caption{Skalierung eines Huffman-Baums} \label{fig:HSCAL}
\end{figure}

Da durch die Integer-Divison Genauigkeit verloren geht, kann es
passieren, dass der resultierende Baum kein Huffman-Baum mehr ist. Ein
Beispiel für diesen Fall ist in Abbildung~\ref{fig:HSCAL} zu sehen.
Nach der Skalierung sind die Häufigkeitszähler nicht mehr aufsteigend
geordnet, wenn man die Knoten von unten links nach oben rechts
durchgeht. Die Lösung dafür ist die vollständige Neuerstellung des
Huffman-Baums nach jeder Skalierung. Bei der Verwendung von 32-Bit
Integern sollte dies nicht sehr häufig passieren.

Die Skalierung der Zähler hat eine Auswirkung auf die Komprimierung,
denn die Symbole, die nach der Skalierung gelesen werden, haben einen
größeren Einfluss auf die Häufigkeitszähler, als die Symbole davor,
deren Häufigkeiten halbiert wurden. Dies kann sogar einen Vorteil
haben, denn die neu gelesenen Symbole können eine größere Abhängigkeit
von den kurz vorher aufgetretenen Symbolen haben als von Symbolen, die
weiter in der Vergangenheit aufgetreten sind.

\subsection{Überlauf der Codeworte}
Ein weiteres Problem ist der mögliche Überlauf der Codewörter. Die
Länge eines Codeworts ist abhängig davon, wie tief ein Blatt-Knoten
für ein Symbol im Huffman-Baum liegt. Bei vielen verschieden
Eingabesymbolen kann der Huffman-Baum große Höhe erreichen.

Aufgrund der Implementierung der Datenstruktur, die den Huffman-Baum
repräsentiert, muss der Kodierer den Pfad von Blattknoten zur Wurzel
durchschreiten, um ein Codewort zu erzeugen. Für die andere Richtung
wäre eine Suche im Baum nach dem zu kodierenden Symbol nötig, was sehr
ineffizient wäre. Weil das Codewort noch umgedreht werden muss, kann
der Kodierer es nicht schon während des Durchlaufens des Baums
ausgeben und muss es zwischenspeichern.

Wenn das Codewort als Integer zwischengespeichert wird, muss beachtet
werden, dass es auch hier zu einem Überlauf kommen kann, wenn das
Codewort zu lang ist. Die einfachste Möglichkeit das Problem zu
umgehen ist die Verwendung einer verketteten Liste, um die Bits des
Codeworts zwischenzuspeichern.  Der einzige Nachteil daran ist der
höhere Speicherbedarf und Rechenaufwand.

Der Dekodierer hat dieses Problem glücklicherweise nicht, denn er
braucht das Codewort nicht zwischenzuspeichern, er kann bei jedem Bit,
dass er liest, sofort zum linken oder rechten Nachfolger des aktuellen
Knotens gehen, bis er einen Blattknoten erreicht hat und das
zugehörige Symbol ausgeben kann.

\section{Anwendungen}
In der Praxis wird die Huffman-Kodierung meist in der adaptiven
Variante verwendet. Ein einfaches Beispiel dafür ist das Unix-Tool
\emph{compact}, welches Dateien mit Hilfe der adaptiven
Huffman-Kodierung komprimiert. Bei vielen anderen
Kompressionsverfahren ist die Huffman-Kodierung Teil eines
mehrstufigen Prozesses der Komprimierung. \cite[S. 234]{Salomon:2010}

Das arithmetische Kodieren ist der größte Konkurrent der
Huffman-Kodierung. Der größte Vorteil über der Huffman-Kodierung ist
eine höhere Kompressionsrate bei den meisten Anwendungsfällen. Auf der
anderen Seite ist das arithmetische Kodieren deutlich aufwendiger und
somit langsamer als die Huffman-Kodierung. Auch ist das adaptive
arithmetische Kodieren nicht so robust wie die adaptive
Huffman-Kodierung, ein einzelner Fehler kann zum falschen Dekodieren
einer großen Datenmenge führen, während sich die adaptive
Huffman-Kodierung schnell wieder korrigiert. \cite{Bookstein:1993}

Die Huffman-Kodierung ist Teil des JPEG-Standards zur verlustbehafteten
Komprimierung von Bilder. Sie wird letzten Schritt der
JPEG-Kompression eingesetzt. Die Huffman-Kodierung wurde aus
Performance- und patentrechtlichen Gründen dem arithmetischen Kodieren
vorgezogen, wobei das arithmeteische Kodieren noch als optionale
Funktion im JPEG-Standard erlaubt ist. \cite{Wallace:1991}

Auch zur verlustbehafteten Audiokompression wird die Huffman-Kodierung
verwendet. Sowohl bei der MP3-Kompression als auch bei der
AAC-Kompression wird die Huffman-Kodierung verwendet. \cite[S. 1055 -- S. 1076]{Salomon:2010}

\subsection{Textkompression}
Die Textkompression scheint eines der idealen Anwendungsfälle für die
Huffman-Kodier\-ung zu sein. Ein Text hat ein eindeutiges Alphabet,
welches normalerweise eine relativ stationäre
Wahrscheinlichkeitsverteilung besitzt. Doch gerade bei Text gibt es
viele Korrelationen zwischen den einzelnen Wörtern und Silben, zum
Beispiel folgt auf das Wort \emph{so} häufig das Wort
\emph{dass}. Solche Eigenschaften werden von der Huffman-Kodierung
nicht ausgenutzt. \cite[S. 74]{Sayood:2006}

Im folgenden Beispiel wird die Kompressionsrate für einen Text
untersucht, der eine Wahrscheinlichkeitsverteilung hat, die der
Buchstabenverteilung der deutschen Sprache entspricht.

In Tabelle~\ref{tab:HGER} ist ein Huffman-Code für die
Wahrscheinlichkeitsverteilung der Buchstaben der deutschen Sprache
Huffman-Code angegeben. Er wurde nach dem Verfahren aus
Abschnitt~\ref{sec:HTREE} erzeugt. Die Wahrscheinlichkeitswerte
stammen aus \cite[S. 10]{Beutelspacher:2005}. Die Umlaute \emph{ä}, \emph{ö}
und \emph{ü} wurden wie \emph{ae}, \emph{oe} und \emph{ue} gezählt.

Um den voraussichtlichen Grad der Komprimierung zu berechnen, wird
zunächst die mittlere Codewort-Länge $L$ berechnet. Sie lässt sich
berechnen, indem die Länge der einzelnen Codewörter mit der
Wahrscheinlichkeit des Symbols für das sie stehen multipliziert wird
und dann die Summe der resultierenden Werte bildet. Folgend ist die
Methode zur Berechnung der mittleren Codewort-Länge nochmal als Formel
angegeben, wobei $n$ die Zahl aller Codewörter ist und die
Funktion \emph{length} die Länge eines Codeworts berechnet:

\begin{equation}
L = \sum_{i=1}^{n} P(a_i) \cdot \mathrm{length}(C(a_i))
\end{equation}

Setzt man nun die Werte für die Daten aus Tabelle~\ref{tab:HGER} ein,
ergibt sich folgender Wert:

\begin{equation}
L = 0.174 \cdot 3 + 0.0978 \cdot 3 + 0.0755 \cdot 4\ \ \cdots\ \ 0.0002 \cdot 11 = 4.1246
\end{equation}

\begin{table}[h]
\centering
\caption{Ein Huffman-Code für die deutsche Sprache}
\begin{tabular}{c|c|l}
$a_i$ & $P(a_i)$ & $C(a_i)$ \\ \hline
E & $0.1740$ & {\tt111} \\
N & $0.0978$ & {\tt001} \\
I & $0.0755$ & {\tt1101} \\
S & $0.0727$ & {\tt1011} \\
R & $0.0700$ & {\tt1010} \\
A & $0.0651$ & {\tt1001} \\
T & $0.0615$ & {\tt0111} \\
D & $0.0508$ & {\tt0101} \\
H & $0.0476$ & {\tt0001} \\
U & $0.0435$ & {\tt0000} \\
L & $0.0344$ & {\tt10001} \\
C & $0.0306$ & {\tt10000} \\
G & $0.0301$ & {\tt01101} \\
M & $0.0253$ & {\tt01001} \\
O & $0.0251$ & {\tt01000} \\
W & $0.0189$ & {\tt110001} \\
B & $0.0189$ & {\tt110010} \\
F & $0.0166$ & {\tt110000} \\
K & $0.0121$ & {\tt011000} \\
Z & $0.0113$ & {\tt1100111} \\
P & $0.0079$ & {\tt1100110} \\
V & $0.0067$ & {\tt0110011} \\
ß & $0.0031$ & {\tt01100100} \\
J & $0.0027$ & {\tt011001011} \\
Y & $0.0004$ & {\tt0110010100} \\
X & $0.0003$ & {\tt01100101011} \\
Q & $0.0002$ & {\tt01100101010}
\end{tabular}
\label{tab:HGER}
\end{table}

Der Huffman-Code aus Tabelle~\ref{tab:HGER} hat somit eine mittlere
Länge von 4.1246 Bit pro Symbol. Wenn man die 27 Buchstaben mit einer
festen Länge kodiert, benötigt man 5 Bit pro Symbol, wobei 5
Codewörter dann nicht belegt sind. Durch die Huffman-Kodierung wird in
diesem Fall also bei einem Text, der eine
Wahrscheinlichkeitsverteilung wie in Tabelle~\ref{tab:HGER} besitzt,
eine Kompressionsrate von $4.1246 : 5 = 82.492 \%$ erreicht. Hierbei
handelt es sich natürlich nur um ein einfaches Beispiel, in der Praxis
kommen sowohl Satzzeichen als auch Groß- und Kleinschreibung hinzu.

\subsection{Verlustfreie Bildkompression}
Die Huffman-Kodierung kann zur verlustfreien Komprimierung von Bildern
verwendet werden. Der einfachste Fall ist ein graustufiges Bild. Die
Symbole des Eingabealphabets wären in diesem Fall die Grauwerte, bei
einer 8-Bit-Abstufung pro Pixel wären das die Werte von 0 bis 255.
Die Komprimierung mit der Huffman-Kodierung ist umso effektiver, je
mehr gleiche Grautöne das zu komprimierende Bild enthält. \cite[S. 72
  f.]{Sayood:2006}

Trotzdem ist mit Kodierung von Bilder mit der Huffman-Kodierung allein
keine gute Kompressionsrate möglich, da die Huffman-Kodierung die
Korrelationen zwischen den Pixeln nicht beachtet. Zum Beispiel hat ein
Pixel, das direkt neben einem anderen Pixel liegt, eine sehr hohe
Wahrscheinlichkeit, die selbe oder eine ähnliche Farbe zu haben als
eine komplett andere.  \cite[S. 217 f.]{Salomon:2010}

\subsection{Verlustfreie Audiokompression}
Ein roher Audiodatenstrom in CD-Qualität und einem Kanal hat eine
Samplerate von 44100~Hz und eine Auflösung von 2 Bytes pro Sample. Das
heißt, dass jede Sekunde 44100 mal die Amplitude eines Audiosignals
gemessen und als 16-Bit Integer gespeichert wurde.

\clearpage
\renewcommand{\refname}{Literaturverzeichnis}
\bibliographystyle{unsrt} \bibliography{adaptive_huffman}
\addcontentsline{toc}{section}{Literaturverzeichnis}

\end{document}
